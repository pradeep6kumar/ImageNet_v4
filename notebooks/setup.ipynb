{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ResNet50v2 trained on ImageNet-1K"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install albumentations --quiet\n",
    "!pip install torchsummary --quiet\n",
    "!pip install tqdm --quiet\n",
    "!pip install matplotlib --quiet\n",
    "!pip install torch-lr-finder --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard Library Imports\n",
    "from collections import OrderedDict\n",
    "\n",
    "# Third Party Imports\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchsummary\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "from torch_lr_finder import LRFinder\n",
    "\n",
    "# Local Imports\n",
    "from datamodule.dataloader import ImageNetDataLoader\n",
    "from datamodule.augmentations import ImageNetAugmentations\n",
    "from model.resnets import ResNet50\n",
    "from train.training_utils import get_lr, train, test, save_checkpoint\n",
    "from utils.visualize import display_loss_and_accuracies\n",
    "from configs.config import TrainingConfig, ModelConfig, DataConfig\n",
    "from configs.aws_setup import setup_training_environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup training environment and get configuration\n",
    "config, train_params = setup_training_environment(debug=True)\n",
    "\n",
    "# Initialize configuration\n",
    "training_config = TrainingConfig()\n",
    "model_config = ModelConfig()\n",
    "data_config = DataConfig()\n",
    "\n",
    "# Update training parameters based on EC2 optimization\n",
    "training_config.batch_size = train_params['batch_size']\n",
    "training_config.num_workers = train_params['num_workers']\n",
    "training_config.gradient_accumulation_steps = config.gradient_accumulation_steps\n",
    "\n",
    "# Device configuration\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmentations\n",
    "\n",
    "Augmentation to be applied during training on ImageNet-1K training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentations = ImageNetAugmentations()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training loader\n",
    "train_loader = ImageNetDataLoader(\n",
    "    beton_path=data_config.train_path,\n",
    "    mode='train',\n",
    "    batch_size=training_config.batch_size,\n",
    "    num_workers=training_config.num_workers,\n",
    "    device=device,\n",
    "    transforms=augmentations.get_transforms('train')\n",
    ")\n",
    "\n",
    "# Create validation loader\n",
    "test_loader = ImageNetDataLoader(\n",
    "    beton_path=data_config.val_path,\n",
    "    mode='val',\n",
    "    batch_size=training_config.batch_size,\n",
    "    num_workers=training_config.num_workers,\n",
    "    device=device\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet50(num_classes=1000)\n",
    "model.to(device)\n",
    "torchsummary.summary(model, (3, data_config.input_size, data_config.input_size), device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate Finder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss Function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(\n",
    "    model.parameters(),\n",
    "    lr=training_config.learning_rate,\n",
    "    weight_decay=training_config.weight_decay\n",
    ")\n",
    "\n",
    "# GradScaler for mixed precision training\n",
    "scaler = GradScaler() if training_config.mixed_precision and torch.cuda.is_available() else None\n",
    "\n",
    "# Compile Model\n",
    "model = torch.compile(model)\n",
    "\n",
    "# Learning rate finder\n",
    "lr_finder = LRFinder(model, optimizer, criterion, device=device)\n",
    "lr_finder.range_test(train_loader, end_lr=10, num_iter=200, step_mode=\"exp\")\n",
    "lr_finder.plot()\n",
    "lr_finder.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LR = 7.19E-02    # Suggested LR\n",
    "STEPS_PER_EPOCH = len(train_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data to plot accuracy and loss graphs\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "train_acc = []\n",
    "test_acc = []\n",
    "learning_rates = []\n",
    "test_incorrect_pred = {'images': [], 'ground_truths': [], 'predicted_vals': []}\n",
    "\n",
    "# Scheduler\n",
    "scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer,\n",
    "                                                max_lr=training_config.learning_rate,\n",
    "                                                steps_per_epoch=STEPS_PER_EPOCH,\n",
    "                                                epochs=training_config.num_epochs,\n",
    "                                                pct_start=5/training_config.epochs,\n",
    "                                                div_factor=100,\n",
    "                                                three_phase=True,\n",
    "                                                final_div_factor=100,\n",
    "                                                anneal_strategy=\"linear\"\n",
    "                                                )\n",
    "\n",
    "# For each epoch\n",
    "for epoch in range(1, training_config.epochs+1):\n",
    "    print(f'Epoch {epoch}')\n",
    "\n",
    "    # Train the model on training dataset and append the training loss and accuracy\n",
    "    correct, processed, train_loss = train(model, device, train_loader, optimizer, criterion, scheduler, scaler, training_config.gradient_accumulation_steps)\n",
    "    train_acc.append(100 * correct / processed)\n",
    "    train_losses.append(train_loss / len(train_loader))\n",
    "    learning_rates.append(get_lr(optimizer))\n",
    "\n",
    "    # Test the model's performance on test dataset and append the training loss and accuracy\n",
    "    correct, test_loss = test(model, device, test_loader, criterion)\n",
    "    test_acc.append(100. * correct / len(test_loader.dataset))\n",
    "    test_losses.append(test_loss)\n",
    "\n",
    "    # Save the model checkpoint\n",
    "    save_checkpoint({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'train_loss': train_loss,\n",
    "        'val_loss': test_loss,\n",
    "    }, f'checkpoint_epoch_{epoch}.pt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Train and Test - Loss and Accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_loss_and_accuracies(train_losses, train_acc, test_losses, test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize One Cycle Policy Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(learning_rates)\n",
    "plt.title(\"Training Learning Rate\")\n",
    "plt.xlabel(\"Training Epochs\")\n",
    "plt.ylabel(\"Learning Rate\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
